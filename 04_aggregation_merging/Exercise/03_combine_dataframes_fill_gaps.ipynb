{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f08c2b",
   "metadata": {},
   "source": [
    "# Combining Datasets with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d848890",
   "metadata": {},
   "source": [
    "#### Creation of a new environment which contains geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f540d",
   "metadata": {},
   "source": [
    "Why's that?  \n",
    "In this notebook we want to use the GeoPandas package, which is based on an _open source project to add support for geographic data to pandas objects._ - in brief: we will have a dataframe with an additional geometric datatype.  \n",
    "\n",
    "Since we usually don't need packages for geospatial data, we don't want to load it everytime we activate our usual nf_base environment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b809e",
   "metadata": {},
   "source": [
    "Let's create a new environment called 'nf_geo', that has 'geopandas' installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959142b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clone the sabbys_base - this may take a few seconds up to two minutes ...\n",
    "# !conda create --yes --name sabby_geo python=3.9 \\\n",
    "#     matplotlib pandas=1.5.2 geopandas scipy seaborn statsmodels scikit-learn ipykernel numpy pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44c097",
   "metadata": {},
   "source": [
    "*(If this last step takes longer than up to a minute and there's a message telling you conda is \"solving environment\", please reach out to us.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc05f4",
   "metadata": {},
   "source": [
    "in order to use our new environment, you have to switch the kernel and select the python kernel of your new nf_geo environment. if it doesn't appear immediately in the list of available kernels, klick on 'select another kernel'..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea79905",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384dbc7",
   "metadata": {},
   "source": [
    "Now that we have our new nf_geo environment, activate it for this jupyter notebook (choose the kernel) and we're ready to import our needed modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d74a4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard import of pandas\n",
    "import pandas as pd\n",
    "\n",
    "# additional import of the geopandas package\n",
    "import geopandas as gpd\n",
    "\n",
    "# numpy, \"numerical python\" - we'll cover this in the following notebooks.\n",
    "import numpy as np\n",
    "\n",
    "# hides warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8afee",
   "metadata": {},
   "source": [
    "## Loading the first dataset\n",
    "The data we'll use is data on bicycle theft crimes at the granular level of Berlin city planning areas, so-called \"LOR\" - \"Lebensweltlich orientierte Räume\", we will stumble over it again later!  \n",
    "This data is provided by Berlin Open Data and collected by the police of Berlin.  \n",
    "\n",
    "### The goal of our analysis is: \n",
    "### Identify areas in Berlin with the most bike thefts and the bike type that has the highest share in thefts.\n",
    "\n",
    "But first things first: We make the data accessible just by loading the .csv-file into a dataframe and get an overview.\n",
    "\n",
    "[Website to datatset -  daten.berlin.de](https://daten.berlin.de/datensaetze/fahrraddiebstahl-berlin)\n",
    "\n",
    "- Licence:\n",
    "    - Creative Commons Namensnennung CC-BY License\n",
    "- Geographical Granularity: \n",
    "    - Berlin\n",
    "- Publisher: \n",
    "    - Polizei Berlin LKA St 14\n",
    "- E Mail: \n",
    "    - onlineredaktion@polizei.berlin.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbdfdf",
   "metadata": {},
   "source": [
    "Next comes a code cell where we define all functions, that we will use in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions\n",
    "\n",
    "def clean_bike_data(df):\n",
    "    # columns name to lower case\n",
    "    df.columns = df.columns.str.lower()\n",
    "    # drop duplicates\n",
    "    df = df.drop_duplicates().copy()\n",
    "    # drop column 'angelegt_am' and 'erfassungsgrund' - irrelevant to us, when and why observation got added to the database.\n",
    "    df.drop(['angelegt_am', 'erfassungsgrund'], axis=1, inplace=True)\n",
    "    #df = df.drop(columns=['angelegt_am', 'erfassungsgrund']) # alternative zu zeile drüber\n",
    "    # we have just 167 attempts and 7 thefts of unknown state in our dataset, so we decide to drop those observations.\n",
    "    df = df[df['versuch'] != 'Ja']\n",
    "    df = df[df['versuch'] != 'Unbekannt']\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop('index', axis=1, inplace=True)\n",
    "    # df.set_index('versuch', inplace=True)\n",
    "    # df.drop(['Ja', 'Unbekannt'], inplace=True)\n",
    "    # change date text string to datetime datatype\n",
    "    df['tatzeit_anfang_datum'] = pd.to_datetime(df['tatzeit_anfang_datum'], format='%d.%m.%Y')\n",
    "    df['tatzeit_ende_datum'] = pd.to_datetime(df['tatzeit_ende_datum'], format='%d.%m.%Y')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe484d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thefts_df_raw = pd.read_csv('../../Data/Fahrraddiebstahl.csv', encoding='latin-1') # proper encoding is necessary here!\n",
    "# make column names lowercase\n",
    "\n",
    "thefts_df_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad79892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the shape, the observations, datatypes and null-counts?\n",
    "thefts_df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2511ee",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the bike data by applying the clean_bike_data function, \n",
    "# that you developed in the data_preparation exercise.\n",
    "\n",
    "# enter code below, assign output to a new variable called thefts_df.\n",
    "thefts_df = clean_bike_data(thefts_df_raw).copy()\n",
    "thefts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd50e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "thefts_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a1e44",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280de336",
   "metadata": {},
   "source": [
    "Now that we're done cleaning our dataset, we'll take a look at the unique values of `art_des_fahrrads`...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ff519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glance at the values of the type of bikes in the dataframe\n",
    "thefts_df.art_des_fahrrads.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea680b",
   "metadata": {},
   "source": [
    "Great! this is the column we need to specifiy the type of bike that has highest share in our bike thefts! to make our analysis on the bike type even easier we show you a popular data transformation in the next code block..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd328f5",
   "metadata": {},
   "source": [
    "The idea is to impute it by using categorical data to so called \"dummy variables\".  \n",
    "Such a variable (aka indicator variable) is a numeric variable representing categorical data by giving each category an own column and assign a 0 or 1 to it.  \n",
    "\n",
    "We'll use this on the \"Art des Fahrrads\" column, the type of bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dummies is a method called on the pandas module - you simply pass in a Pandas Series \n",
    "# or DataFrame, and it will convert a categorical variable into dummy/indicator variables. \n",
    "# The idea of dummy coding is to convert each category into a new column, and assign a 1 or 0 to the column.\n",
    "# This can be an important step during data preparation for machine learning.\n",
    "\n",
    "# creating a dataset of type of bike dummy variables.\n",
    "biketype_dummies = pd.get_dummies(thefts_df.art_des_fahrrads, prefix='type')\n",
    "biketype_dummies.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5697622",
   "metadata": {},
   "source": [
    "This looks good but now `biketype_dummies` is a different dataframe than `thefts_df`, we have our first use case where we need to merge to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af863538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------- DF 1 : thefts_df -----------------\")\n",
    "display(thefts_df.head(3))\n",
    "print(\"--------- DF 2 : biketype_dummies ----------\")\n",
    "display(biketype_dummies.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4da48",
   "metadata": {},
   "source": [
    "Discuss with your group partner....on what key could you combine the two datasets? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081573e",
   "metadata": {},
   "source": [
    "## Combining dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc4223",
   "metadata": {},
   "source": [
    "In this case we can only join on the index, there is no other unique identifier in the data. As we want to combine via index, which method is preferred `join()` or `merge()`? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab371309",
   "metadata": {},
   "source": [
    "### Join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f40ed",
   "metadata": {},
   "source": [
    "Now let's look at the join() method. It joins on indices by default and is called on a dataframe instance. This means that we can simply join our bike type dummies dataframe back to our original bike thefts dataframe with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining columns of another DataFrame using the join() method.\n",
    "join_df = thefts_df.join(biketype_dummies)\n",
    "join_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's have a look at the columns of our newly assigned dataframe\n",
    "join_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c29169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's store our final output in a new variable and create a new copy of the dataset\n",
    "thefts_df_enriched = join_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06604ea4",
   "metadata": {},
   "source": [
    "The arguments of `.join` are the following:\n",
    "```\n",
    "DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "```\n",
    "The documentation refers to the second dataframe as 'other', which the documentations of the other combining methods often refer to as 'right'.\n",
    "With how we can specify which join method we want to use.\n",
    "\n",
    "If we want to join using a common column, we need to set this column to be the index in both dataframes. The joined DataFrame will have the common column as its index.\n",
    "```\n",
    "df.set_index('column_name').join(other.set_index('column_name'))\n",
    "```\n",
    "Another option to join using a common column is to use the on parameter. This method preserves the original DataFrame’s index in the result.\n",
    "```\n",
    "df.join(other.set_index('column_name'), on='column_name')\n",
    "```\n",
    "See the documentation for more information.\n",
    "\n",
    "The how argument to merge specifies which keys are included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names:\n",
    "\n",
    "Merge/Join in Pandas | SQL Join Name | Description\n",
    "---|---|---\n",
    "left| LEFT OUTER JOIN | Use keys from left frame only\n",
    "right | RIGHT OUTER JOIN | Use keys from right frame only\n",
    "outer | FULL OUTER JOIN | Use union of keys from both frames\n",
    "inner | INNER JOIN | Use intersection of keys from both frames\n",
    "\n",
    "\n",
    "You can also think of it as set theory and use Venn diagrams to illustrate what happens in each method.\n",
    "\n",
    "![Join Methods](../../images/join_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92ed7e",
   "metadata": {},
   "source": [
    "### Merge()\n",
    "Let's look at the `merge()` method. Merge combines dataframes on common columns by default and can be used via the pandas module AND called on a dataframe instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d78c9",
   "metadata": {},
   "source": [
    "The arguments of `.merge` are the following: \n",
    "````\n",
    "DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False,   \n",
    "suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "````\n",
    "See the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6c5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since in both dataframes, we need a common column.\n",
    "# Let's use the index column as the one to merge on:\n",
    "# moved this command to the function on top\n",
    "thefts_df_ind = thefts_df.reset_index()\n",
    "biketype_dummies_ind = biketype_dummies.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa052e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result - you will see a new column called index in the dataframe\n",
    "thefts_df_ind.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa201cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result - you will see a new column called index in the dataframe\n",
    "biketype_dummies_ind.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bde0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the quality_dummies df on the thefts_df instance on the common column 'index'\n",
    "merge_df1 = thefts_df_ind.merge(biketype_dummies_ind, on='index')\n",
    "merge_df1.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or another way: Merge the two dataframes via the pandas module on the common column 'index'\n",
    "merge_df2 = pd.merge(thefts_df_ind, biketype_dummies_ind, on='index')\n",
    "merge_df2.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f26e7",
   "metadata": {},
   "source": [
    "The second approach with the `merge()`-method was for showing you the syntax, we will not use the result anywhere. For the rest of the notebook we will work with the combined dataframe from the `join()`-method, remember we called it `thefts_df_enriched`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6e85e",
   "metadata": {},
   "source": [
    "## Combining multiple data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92adda7",
   "metadata": {},
   "source": [
    "Remember we initially said, we wanted to be able to identify areas in Berlin with the most bike thefts?  \n",
    "So far, we can't.  \n",
    "\n",
    "We have a lot of features describing the actual bike thefts, but we have nothing to really spot the area where it happens. The only thing we have in our dataframe is this suspicious \"LOR\" - so we have to do some research on it, if and how we can use it ...  \n",
    "\n",
    "The [dataset description](https://www.berlin.de/polizei/_assets/dienststellen/lka/datensatzbeschreibung.pdf) at Berlin Open Data tells us about the LOR column:\n",
    "- Kennung des Planungsraums, 8-stellig\n",
    "- Raumhierarchie lebensweltlich orientierte Räume (LOR) der Senatsverwaltung für Stadtentwicklung und\n",
    "Wohnen\n",
    "\n",
    "Wow. _Raumhierarchie lebensweltlich orientierte Räume_ - that's where you know you deal with authorities. \n",
    "Since we don't have any Ideas what that means, we google it and find, that at the Website of [stadtentwicklung.berlin.de](https://www.stadtentwicklung.berlin.de/planen/basisdaten_stadtentwicklung/lor/de/download.shtml) there are LOR associated vector data files, .shp \"shapefiles\". So we have a look at them, too ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a0dcf",
   "metadata": {},
   "source": [
    "We now access the shapefiles and try to combine them with our biketheft data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894590a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a geodataframe based on the shapefile\n",
    "gdf = gpd.GeoDataFrame.from_file('../../Data/LOR_SHP_2021/lor_plr.shp')\n",
    "gdf.columns = gdf.columns.str.lower()\n",
    "gdf.head(5)\n",
    "\n",
    "# attention! if you had troubles installing geopandas, you won't be able to plot the ploygons!\n",
    "# however, you can still proceed with the rest of the analysis if you read in this csv instead:\n",
    "# gdf = pd.read_csv('../../data/shapefile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e33ecd",
   "metadata": {},
   "source": [
    "So we see, this gave us a dataframe with obviously the LOR as plr_id, the district name and the geometrical shape of the area as a polygon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02642044",
   "metadata": {},
   "source": [
    "##### Polygon? What was that again?\n",
    "\n",
    "<img src=\"../../images/geometries.jpg\" alt=\"geometries\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c75e1",
   "metadata": {},
   "source": [
    "So, those polygons should give us areas of Berlin. Let's give it a try: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the geometries\n",
    "berlin = gdf.plot(color='grey', figsize=(12, 12));\n",
    "# optional: comment out the next line to highlight a particular suburb in red...\n",
    "gdf[gdf['plr_name'] == 'Waßmannsdorfer Chaussee'].plot(ax=berlin, color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kepler for beautiful visuals: https://kepler.gl/,\n",
    "can be used in jupyter notebooks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed600ba",
   "metadata": {},
   "source": [
    "That somehow looks like Berlin which makes us quite confident to proceed to try to merge the sets, since our bike theft data is not yet inside our geodataframe (or vice versa) - those are still two seperate data sets.  \n",
    "\n",
    "So - we need to have a look at the column that allow us to merge ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike thefts lor column\n",
    "thefts_df.lor.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ab8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geodataframe lor column\n",
    "gdf.plr_id.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cebe37",
   "metadata": {},
   "source": [
    "Not that easy, again.  \n",
    "- The column 'lor' in the bike theft data is an integer.  \n",
    "- Integers as numeric values can't have leading zeros.  \n",
    "- That's why it sometimes is 8 digits, sometimes is just 7 digits long - it then misses a leading 0 - we need to impute!  \n",
    "\n",
    "In the geodataframe, the lor column is an object, which means a string in this case.  \n",
    "Feel free to have a closer look ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two DataFrames by plr_id in format INT\n",
    "# (so converting in gdf the type of 'plr_id' into int, and just renaming the column 'lor' in thefts)\n",
    "\n",
    "gdf_int = gdf.copy()\n",
    "thefts_df_enriched_int = thefts_df_enriched.copy()\n",
    "\n",
    "gdf_int['plr_id_int'] = gdf_int['plr_id'].astype(int)\n",
    "thefts_df_enriched_int['plr_id_int'] = thefts_df_enriched_int['lor']\n",
    "\n",
    "gdf_biketheft_int = pd.merge(thefts_df_enriched_int, gdf_int, on='plr_id_int', how='outer')\n",
    "gdf_biketheft_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b031299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate maximum and minimum length for numeric columns\n",
    "\n",
    "# The .astype(str) converts the numeric values to strings.\n",
    "# The .apply(len) calculates the length of each string.\n",
    "\n",
    "max_lengths = thefts_df_enriched['lor'].astype(str).apply(len).max()\n",
    "min_lengths = thefts_df_enriched['lor'].astype(str).apply(len).min()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum length: {max_lengths}\")\n",
    "print(f\"Minimum length: {min_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the lor column datatype to string and call new column 'lor_str'\n",
    "thefts_df_enriched['lor_str']= thefts_df_enriched['lor'].astype(str)\n",
    "thefts_df_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thefts_df_enriched.lor_str.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdee77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill leading gaps up to 8 characters with zeros and call the new column accordingly to the geodataframe\n",
    "# example 3400723 to 03400723\n",
    "\n",
    "# The zfill() method is used to pad the left side of strings with zeros.\n",
    "# It ensures that the resulting string has a specified minimum length by adding leading zeros if necessary.\n",
    "thefts_df_enriched['plr_id'] = thefts_df_enriched['lor_str'].str.zfill(8)\n",
    "thefts_df_enriched.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before dropping not needed columns, let's check\n",
    "thefts_df_enriched[['plr_id','lor_str']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c80e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_2 = thefts_df_enriched['plr_id'].astype(str).apply(len).max()\n",
    "min_lengths_2 = thefts_df_enriched['plr_id'].astype(str).apply(len).min()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum length: {max_lengths_2}\")\n",
    "print(f\"Minimum length: {min_lengths_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2ee47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...looks good in our sample check :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12909287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping no longer needed columns\n",
    "thefts_df_enriched.drop(columns=['lor', 'lor_str'], inplace=True)\n",
    "thefts_df_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a49e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with the geodataframe\n",
    "display(thefts_df_enriched[['art_des_fahrrads','delikt','plr_id']].head())\n",
    "display(gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef65cb",
   "metadata": {},
   "source": [
    "Now, we are able to merge our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78857af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes 'gdf' and 'thefts_df_enriched' on the plr_id columns\n",
    "# call new dataframe 'gdf_biketheft'\n",
    "# what type of merge do we want to do? in our case we only want to keep the rows \n",
    "# that match with a plr in  gdf, so we perform 'inner' merge\n",
    "\n",
    "gdf_biketheft = pd.merge(thefts_df_enriched, gdf, on='plr_id', how='outer')\n",
    "gdf_biketheft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db496c6",
   "metadata": {},
   "source": [
    "And so, we are finally able to infer infer the are with the most bikes stolen  \n",
    "by aggregating count of thefts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36d5ba",
   "metadata": {},
   "source": [
    "### How many bikethefts per postcode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft.type_Damenfahrrad.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e6088",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft.groupby('plr_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting thefts in areas\n",
    "# note: we need .reset_index(name='thefts') to convert the output back to a pandas dataframe.\n",
    "df_plr_group_thefts = gdf_biketheft.groupby('plr_id')['versuch'].count().reset_index(name='thefts')\n",
    "# showing new dataframe with plr_id and aggregated count of thefts\n",
    "df_plr_group_thefts #.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plr_group_thefts[df_plr_group_thefts['thefts']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276f0bc",
   "metadata": {},
   "source": [
    "### What's the average monetary value of the thefts per postcode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1738e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count thefts in areas and store the result in a new df called 'df_plr_group_mean',\n",
    "# name the column with avg monetary value 'avg_amount'\n",
    "df_plr_group_mean = gdf_biketheft.groupby('plr_id').agg(avg_amount=('schadenshoehe', 'mean'))\n",
    "\n",
    "# showing new dataframe with plr_id and aggregated mean of thefts\n",
    "df_plr_group_mean #.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many thefts per bike type per postcode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of thefts per bike type per post_code\n",
    "# and store the result in a new df called 'df_bike_types_count'\n",
    "\n",
    "#df_bike_types_count = gdf_biketheft.groupby(['plr_id', 'art_des_fahrrads'])['art_des_fahrrads'].count()\n",
    "#df_bike_types_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plr_group_thefts.thefts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e50a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bike_types_count_2 = gdf_biketheft.groupby(['plr_id'])\\\n",
    "    .agg({\n",
    "        'type_Damenfahrrad': 'sum', \n",
    "        'type_Fahrrad': 'sum', \n",
    "        'type_Herrenfahrrad': 'sum', \n",
    "        'type_Kinderfahrrad': 'sum', \n",
    "        'type_Lastenfahrrad': 'sum',\n",
    "        'type_Mountainbike': 'sum',\n",
    "        'type_Rennrad': 'sum',\n",
    "        'type_diverse Fahrräder':'sum'\n",
    "        }).reset_index()\n",
    "df_bike_types_count_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging the aggregates into the initial geodataframe\n",
    "\n",
    "# merge 'df_plr_group_thefts' and 'gdf' and save result as 'gdf_biketheft_1'\n",
    "gdf_biketheft_1 = pd.merge(df_plr_group_thefts, gdf, on='plr_id')\n",
    "\n",
    "# merge 'gdf_biketheft_1' and 'df_plr_group_mean' and save result as 'gdf_biketheft_2'\n",
    "gdf_biketheft_2 = pd.merge(gdf_biketheft_1, df_plr_group_mean, on='plr_id')\n",
    "\n",
    "# merge 'gdf_biketheft_2' and 'df_bike_types_count' and save result as 'gdf_biketheft'\n",
    "gdf_biketheft_res = pd.merge(gdf_biketheft_2, df_bike_types_count_2, on='plr_id')\n",
    "gdf_biketheft_res #.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd041deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res.thefts.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res[gdf_biketheft_res['thefts']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res['plr_id'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_plr_group_thefts.head(1))\n",
    "display(gdf.head(1))\n",
    "display(df_plr_group_mean.head(1))\n",
    "display(df_bike_types_count_2.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share of thefts per bike type per postcode\n",
    "\n",
    "In order to answer one of our main questions we need to calculate the shares for each bike type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the package numpy to round some numbers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give nr of thefts per bike type and total thefts, calculcate the share.\n",
    "# name the new columns in the following form:\n",
    "# share_Damenfahrrad = nr type_Damenfahrrad / nr thefts\n",
    "\n",
    "#gdf_biketheft_res['share_Damenfahrrad'] = (gdf_biketheft_res['type_Damenfahrrad'] / gdf_biketheft_res['thefts']).round(2)\n",
    "#gdf_biketheft_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7acfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_types = ['Damenfahrrad', 'Fahrrad', 'Herrenfahrrad', 'Kinderfahrrad', 'Lastenfahrrad', 'Mountainbike', 'Rennrad', 'diverse Fahrräder']\n",
    "\n",
    "for bike_type in bike_types:\n",
    "    gdf_biketheft_res[f'share_{bike_type}'] = (gdf_biketheft_res[f'type_{bike_type}'] / gdf_biketheft_res['thefts']).round(2)\n",
    "\n",
    "gdf_biketheft_res.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da918f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res.type_Herrenfahrrad.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at one example to double check our calculations\n",
    "gdf_biketheft_res[['plr_id','type_Herrenfahrrad','thefts','share_Herrenfahrrad']].sort_values('thefts', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d70713fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think of more ways to check your calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok great now we need to calculate the bike type with the highest share for each row...\n",
    "# we want to store this information in a new column called 'type_highest_share'\n",
    "# tip: search of a pandas function that fullfills this task! \n",
    "\n",
    "## with idmax the order of the columns matters: when two columns have the same value, the first one will be picked\n",
    "\n",
    "gdf_biketheft_res['type_highest_share'] = gdf_biketheft_res[[\n",
    "    'share_Herrenfahrrad',  'share_Damenfahrrad',\n",
    "     'share_Kinderfahrrad', 'share_Lastenfahrrad',\n",
    "     'share_Mountainbike', 'share_Fahrrad',\n",
    "    'share_Rennrad', 'share_diverse Fahrräder'\n",
    "    ]].idxmax(axis=1)\n",
    "gdf_biketheft_res.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Filter for the following columns\n",
    "\n",
    "gdf_biketheft_res[[\n",
    "    'plr_name','share_Rennrad','share_Fahrrad','share_Damenfahrrad','share_Herrenfahrrad','type_highest_share'\n",
    "    ]].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6602f3",
   "metadata": {},
   "source": [
    "ok great! We have calculated for each post code what type of bike was stolen the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a5e65c",
   "metadata": {},
   "source": [
    "## Retrieve the bike type with the highest share of thefts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cfb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike type with highest share of thefts\n",
    "gdf_biketheft_res.groupby('type_highest_share')['thefts'].count() #.sort_values().tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res.groupby('type_highest_share').size() #.sort_values().tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1875e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res['type_highest_share'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef91c0b",
   "metadata": {},
   "source": [
    "It is __Herrenfahrrad with 438 thefts__ in the observed timeframe!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00f4779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns = ['share_Herrenfahrrad','share_Damenfahrrad','share_Kinderfahrrad',\n",
    "                                                     'share_Lastenfahrrad','share_Mountainbike','share_Rennrad',\n",
    "                                                     'share_diverse Fahrräder','share_Fahrrad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fahrrad_max_share = gdf_biketheft_res[['share_Herrenfahrrad','share_Damenfahrrad','share_Kinderfahrrad',\n",
    "                                                     'share_Lastenfahrrad','share_Mountainbike','share_Rennrad',\n",
    "                                                     'share_diverse Fahrräder','share_Fahrrad']].max(axis=1)\n",
    "fahrrad_max_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fahrrad_type = [ \",\".join([col for col in my_columns if gdf_biketheft_res.loc[i,col]==mx]) for i,mx in fahrrad_max_share.items()]\n",
    "fahrrad_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cdb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_df = pd.DataFrame({\"share\":fahrrad_max_share, \"kind\":fahrrad_type}, index=gdf_biketheft_res.index)\n",
    "share_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_df['fahrrad_types'] = share_df['kind'].str.split(',')\n",
    "share_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_df_exploded = share_df.explode('fahrrad_types')\n",
    "share_df_exploded.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we used the columns of gdf_bikethefts_res, and we also thefts and type_Herrenfahrrad ... as results below\n",
    "\n",
    "share_df_exploded[share_df_exploded['fahrrad_types']=='thefts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e95b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_df_exploded['fahrrad_types'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755fefe",
   "metadata": {},
   "source": [
    "## Retrieve the postcode with the highest share of thefts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_biketheft_res['thefts'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the postcode with the highest share of thefts\n",
    "\n",
    "gdf_biketheft_res[gdf_biketheft_res.thefts == gdf_biketheft_res.thefts.max()][['plr_name', 'avg_amount', 'thefts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a029d7e",
   "metadata": {},
   "source": [
    "It is __Alt-Treptow with 501 thefts__ in the observed timeframe with an average theft amount of 791 Euro!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef773c",
   "metadata": {},
   "source": [
    "Congratulations!  \n",
    "You made it through another intense notebook - but we hope the little excursions brought some fun ..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4771abb73651cc71498e03f3559c7e0f15f38d5124065b3832974a7bbffea7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nf_base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
