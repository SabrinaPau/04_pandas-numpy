{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f08c2b",
   "metadata": {},
   "source": [
    "# Combining Datasets & Pivot Tables with Pandas\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "At the end of this notebook you should be able to\n",
    "- combine DataFrames with Pandas\n",
    "- describe the different joining methods (how to join DataFrames)\n",
    "- create pivot tables with Pandas\n",
    "- clone (\"copy\") conda environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48076c",
   "metadata": {},
   "source": [
    "Pandas functions that allow us to combine two sets of data include the use of `pd.merge()`, `df.join()`, `df.merge()`, and `pd.concat()`. For the most part, these do largely the same things (although you'll notice the slight syntax difference with `merge()` and `concat()` being able to be called via the Pandas module and `merge()` and `join()` being able to be called on a DataFrame instance).   \n",
    "There are some cases where one of these might be better than another in terms of writing less code or performing some kind of data combination in an easier way. The major differences between these, though, largely depend on what they do by default when you try to combine different data. By default, `merge()` looks to join on common columns, `join()` on common indices, and `concat()` by just appending on a given axis.\n",
    "\n",
    "You can find more detail about the differences between all three of these in the [docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html). We'll look at some examples below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d848890",
   "metadata": {},
   "source": [
    "## But first: cloning your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f540d",
   "metadata": {},
   "source": [
    "Why's that?  \n",
    "In this notebook we want to use the GeoPandas package, which is based on an _open source project to add support for geographic data to pandas objects._ - in brief: we will have a dataframe with an additional geometric datatype.  \n",
    "\n",
    "Since we usually don't need packages for geospatial data, we don't want to load it everytime we activate our usual nf_base environment.  \n",
    "The good thing is:  \n",
    "In conda, it is possible to easily copy, or better, clone an existing environment,  \n",
    "so we will make use of this, create a new environment called _nf_geo_ based on _nf_base_, add the GeoPandas package to it and then use this new environment for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b809e",
   "metadata": {},
   "source": [
    "To do so, let's clone your hopefully properly set up nf_base environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959142b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the nf_base - this may take a few seconds up to two minutes ...\n",
    "!conda create --clone nf_base --name nf_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c5608",
   "metadata": {},
   "source": [
    "Since GeoPandas is available via the conda forge channel, you may have to enable this channel first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add conda forge to your conda channels\n",
    "!conda config --add channels conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d5a0a",
   "metadata": {},
   "source": [
    "Then install the package directly into the newly built environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This somehow means \"conda install from conda forge into the nf_geo environment (-n nf_geo) not asking for confirmation (-y): package geopandas\" \n",
    "!conda install -c conda-forge -n nf_geo -y geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44c097",
   "metadata": {},
   "source": [
    "*(If this last step takes longer than up to a minute and there's a message telling you conda is \"solving environment\", please reach out to us.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea79905",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384dbc7",
   "metadata": {},
   "source": [
    "Now that we have our new nf_geo environment, activate it for this jupyter notebook (choose the kernel) and we're ready to import our needed modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard import of pandas\n",
    "import pandas as pd\n",
    "\n",
    "# additional import of the geopandas package\n",
    "import geopandas as gpd\n",
    "\n",
    "# numpy, \"numerical python\" - we'll cover this in the following notebooks.\n",
    "import numpy as np\n",
    "\n",
    "# hides warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8afee",
   "metadata": {},
   "source": [
    "## Loading the first dataset\n",
    "The data we'll use is data on bicycle theft crimes at the granular level of Berlin city planning areas, so-called \"LOR\" - \"Lebensweltlich orientierte RÃ¤ume\", we will stumble over it again later!  \n",
    "This data is provided by Berlin Open Data and collected by the police of Berlin.  \n",
    "\n",
    "### The goal is: To be able to identify areas in Berlin with the most bike thefts or the highest theft amounts  \n",
    "\n",
    "But first things first: We make the data accessible just by loading the .csv-file into a dataframe and get an overview.\n",
    "\n",
    "[Website to datatset -  daten.berlin.de](https://daten.berlin.de/datensaetze/fahrraddiebstahl-berlin)\n",
    "\n",
    "- Licence:\n",
    "    - Creative Commons Namensnennung CC-BY License\n",
    "- Geographical Granularity: \n",
    "    - Berlin\n",
    "- Publisher: \n",
    "    - Polizei Berlin LKA St 14\n",
    "- E Mail: \n",
    "    - onlineredaktion@polizei.berlin.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe484d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thefts_df = pd.read_csv('data/Fahrraddiebstahl.csv', encoding='latin-1') # proper encoding is necessary here!\n",
    "thefts_df.columns = thefts_df.columns.str.lower()  # make column names lowercase\n",
    "thefts_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad79892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the shape, the observations, datatypes and null-counts?\n",
    "thefts_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ba7f3",
   "metadata": {},
   "source": [
    "Let's quickly think about cleaning our data:\n",
    "\n",
    "- drop duplicates? inspect!\n",
    "- drop column 'angelegt_am' and 'erfassungsgrund' - irrelevant to us\n",
    "- column 'versuch': inspect!  \n",
    "- column 'tatzeit_anfang_datum': change date string to datetime format  \n",
    "- column 'tatzeit_anfang_ende': change date string to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a304929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect duplicates\n",
    "thefts_df[thefts_df.duplicated(keep=False)].sort_values(by=['tatzeit_anfang_datum', 'schadenshoehe']).tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f13222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the specifications of the duplicates indicate that they are implausible, so we drop them.\n",
    "# drop duplicates (rows by default)\n",
    "thefts_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column 'angelegt_am' and 'erfassungsgrund' - irrelevant to us, when and why observation got added to the database.\n",
    "thefts_df.drop(columns='angelegt_am', inplace=True)\n",
    "thefts_df.drop(columns='erfassungsgrund', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique values holds the column of the attempts?\n",
    "thefts_df.versuch.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and what is the count of those categories?\n",
    "thefts_df.versuch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have just 167 attempts and 7 thefts of unknown state in our dataset, so we decide to drop those observations.\n",
    "thefts_df = thefts_df[thefts_df.versuch != 'Ja']\n",
    "thefts_df = thefts_df[thefts_df.versuch != 'Unbekannt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date text string to datetime datatype\n",
    "thefts_df['tatzeit_anfang_datum'] = pd.to_datetime(thefts_df['tatzeit_anfang_datum'])\n",
    "thefts_df['tatzeit_ende_datum'] = pd.to_datetime(thefts_df['tatzeit_ende_datum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that the dates are not only strings anymore, we can have a look at the timeframe\n",
    "thefts_df.tatzeit_anfang_datum.min(), thefts_df.tatzeit_ende_datum.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... or can even do calculations on the date fields\n",
    "thefts_df.tatzeit_ende_datum.max() - thefts_df.tatzeit_anfang_datum.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the new datatypes\n",
    "thefts_df[['tatzeit_anfang_datum', 'tatzeit_ende_datum']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280de336",
   "metadata": {},
   "source": [
    "Now that we're done cleaning our dataset, the idea is to impute it by using categorical data to so called \"dummy variables\".  \n",
    "Such a variable (aka indicator variable) is a numeric variable representing categorical data by giving each category an own column and assign a 0 or 1 to it.  \n",
    "\n",
    "We'll use this on the \"Art des Fahrrads\" column, the type of bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ff519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glance at the values of the type of bikes in the dataframe\n",
    "thefts_df.art_des_fahrrads.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dummies is a method called on the pandas module - you simply pass in a Pandas Series \n",
    "# or DataFrame, and it will convert a categorical variable into dummy/indicator variables. \n",
    "# The idea of dummy coding is to convert each category into a new column, and assign a 1 or 0 to the column.\n",
    "# This can be an important step during data preparation for machine learning.\n",
    "\n",
    "# creating a dataset of type of bike dummy variables.\n",
    "biketype_dummies = pd.get_dummies(thefts_df.art_des_fahrrads, prefix='type')\n",
    "biketype_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081573e",
   "metadata": {},
   "source": [
    "## Combining dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aeae0b",
   "metadata": {},
   "source": [
    "### Join()\n",
    "Now let's look at the `join()` method. It joins on indices by default and is called on a dataframe instance. This means that we can simply join our bike type dummies dataframe back to our original bike thefts dataframe with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining columns of another DataFrame using the join() method.\n",
    "join_df = thefts_df.join(biketype_dummies)\n",
    "join_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the columns of our newly assigned dataframe\n",
    "join_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af585f9",
   "metadata": {},
   "source": [
    "The arguments of `.join` are the following: \n",
    "````\n",
    "DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "````\n",
    "The documentation refers to the second dataframe as 'other', which the documentations of the other combining methods often refer to as 'right'.  \n",
    "With `how` we can specify which join method we want to use.\n",
    "\n",
    "If we want to join using a common column, we need to set this column to be the index in both dataframes. The joined DataFrame will have the common column as its index.\n",
    "\n",
    "```\n",
    "df.set_index('column_name').join(other.set_index('column_name'))\n",
    "```\n",
    "\n",
    "Another option to join using a common column is to use the on parameter. This method preserves the original DataFrameâs index in the result.\n",
    "```\n",
    "df.join(other.set_index('column_name'), on='column_name')\n",
    "```\n",
    "See the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19159e7",
   "metadata": {},
   "source": [
    "The how argument to merge specifies which keys are included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names:\n",
    "\n",
    "Merge method | SQL Join Name | Description\n",
    "---|---|---\n",
    "left| LEFT OUTER JOIN | Use keys from left frame only\n",
    "right | RIGHT OUTER JOIN | Use keys from right frame only\n",
    "outer | FULL OUTER JOIN | Use union of keys from both frames\n",
    "inner | INNER JOIN | Use intersection of keys from both frames\n",
    "\n",
    "\n",
    "You can also think of it as set theory and use Venn diagrams to illustrate what happens in each method.\n",
    "\n",
    "![Join Methods](./images/join_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92ed7e",
   "metadata": {},
   "source": [
    "### Merge()\n",
    "Let's look at the `merge()` method. Merge combines dataframes on column columns by default and can be used via the pandas module AND called on a dataframe instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d78c9",
   "metadata": {},
   "source": [
    "The arguments of `.merge` are the following: \n",
    "````\n",
    "DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False,   \n",
    "suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "````\n",
    "See the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since in both dataframes, we need a common column.\n",
    "# Let's use the index column as the one to merge on:\n",
    "thefts_df_ind = thefts_df.reset_index()\n",
    "biketype_dummies_ind = biketype_dummies.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa052e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result - you will see a new column called index in the dataframe\n",
    "thefts_df_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa201cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result - you will see a new column called index in the dataframe\n",
    "biketype_dummies_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bde0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the quality_dummies df on the thefts_df instance on the common column 'index'\n",
    "merge_df1 = thefts_df_ind.merge(biketype_dummies_ind, on='index')\n",
    "merge_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca69d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or another way: Merge the two dataframes via the pandas module on the common column 'index'\n",
    "merge_df2 = pd.merge(thefts_df_ind, biketype_dummies_ind, on='index')\n",
    "merge_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6076cf",
   "metadata": {},
   "source": [
    "### Concat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8766ca4",
   "metadata": {},
   "source": [
    "Let's now look at concat.\n",
    "`````\n",
    "pandas.concat(objs, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None,    \n",
    "verify_integrity=False, sort=False, copy=True)\n",
    "`````\n",
    "Different from join and merge, which by default operate on columns, concat can define whether to operate on columns or rows.\n",
    "\n",
    "See the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37001e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([biketype_dummies, thefts_df], axis=1)\n",
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb64904",
   "metadata": {},
   "source": [
    "In the images below, you can see the differences, if axis is set as 0 or 1.\n",
    "\n",
    "**Concat with axis=0:**\n",
    "![Concat Axis 0](./images/concat_axis_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Concat with axis=1:**\n",
    "![Concat Axis 1](./images/concat_axis_1.png)\n",
    "\n",
    "(The pictures were part of [this](https://towardsdatascience.com/python-pandas-dataframe-join-merge-and-concatenate-84985c29ef78) blog post.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d814c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299f39a",
   "metadata": {},
   "source": [
    "## Check your understanding\n",
    "\n",
    "Leaving bike thefts aside,  \n",
    "1. Please join the two given dataframes (df1 and df2) along rows and merge with the third (df3) dataframe along the common column id.  \n",
    "If any key combinations are not present, these should be filled with NaNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "         'name': ['Erika Raaf', 'Nadja Berens', 'Florentin Kleist', 'Dorothea Eibl', 'Gerhard Bihlmeier'], \n",
    "        'subject': ['Math', 'Biology', 'Biology', 'English', 'Philosophy']})\n",
    "df2 = pd.DataFrame({\n",
    "        'student_id': ['S6', 'S7', 'S8'],\n",
    "        'name': ['Jens HÃ¼ls', 'Vera Kagan', 'Paula Brodersen'], \n",
    "        'subject': ['Math', 'Math', 'Social Science']})\n",
    "df3 = pd.DataFrame({\n",
    "        'student_id': ['S1', 'S2', 'S3', 'S4', 'S5', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13'],\n",
    "        'marks': [23, 45, 12, 67, 21, 55, 33, 14, 56, 83, 88, 12]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8dd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a508f7e",
   "metadata": {},
   "source": [
    "2. You have received some weather data (temperature) of the last year. For each month the average temperature was measured, only for a few months the maximum temperature could be measured. Anyway, you want to combine these two data without losing any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mean_data = {'Mean TemperatureF': [53.1, 70., 34.93548387, 28.71428571, 32.35483871, 72.87096774, 70.13333333, 35., 62.61290323, 39.8, 55.4516129 , 63.76666667],\n",
    "                     'Month': ['Apr', 'Aug', 'Dec', 'Feb', 'Jan', 'Jul', 'Jun', 'Mar', 'May', 'Nov', 'Oct', 'Sep']}\n",
    "weather_max_data = {'Max TemperatureF': [68, 89, 91, 84], 'Month': ['Jan', 'Apr', 'Jul', 'Oct']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f42814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e1800a8",
   "metadata": {},
   "source": [
    "\n",
    "3. (Extra question: Can you fill in the average max. Temperature for the missing values in the Column `Max TemperatureF`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee2f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f4e0518",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6e85e",
   "metadata": {},
   "source": [
    "## Combining multiple data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92adda7",
   "metadata": {},
   "source": [
    "Remember we initially said, we wanted to be able to identify areas in Berlin with the most bike thefts?  \n",
    "So far, we can't.  \n",
    "\n",
    "We have a lot of features describing the actual bike thefts, but we have nothing to really spot the area where it happens. The only thing we have in our dataframe is this suspicious \"LOR\" - so we have to do some research on it, if and how we can use it ...  \n",
    "\n",
    "The [dataset description](https://www.berlin.de/polizei/_assets/dienststellen/lka/datensatzbeschreibung.pdf) at Berlin Open Data tells us about the LOR column:\n",
    "- Kennung des Planungsraums, 8-stellig\n",
    "- Raumhierarchie lebensweltlich orientierte RÃ¤ume (LOR) der Senatsverwaltung fÃ¼r Stadtentwicklung und\n",
    "Wohnen\n",
    "\n",
    "Wow. _Raumhierarchie lebensweltlich orientierte RÃ¤ume_ - that's where you know you deal with authorities. \n",
    "Since we don't have any Ideas what that means, we google it and find, that at the Website of [stadtentwicklung.berlin.de](https://www.stadtentwicklung.berlin.de/planen/basisdaten_stadtentwicklung/lor/de/download.shtml) there are LOR associated vector data files, .shp \"shapefiles\". So we have a look at them, too ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a0dcf",
   "metadata": {},
   "source": [
    "We now access the shapefiles and try to combine them with our biketheft data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894590a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a geodataframe based on the shapefile\n",
    "gdf = gpd.GeoDataFrame.from_file('data/LOR_SHP_2021/lor_plr.shp')\n",
    "gdf.columns = gdf.columns.str.lower()\n",
    "gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e33ecd",
   "metadata": {},
   "source": [
    "So we see, this gave us a dataframe with obviously the LOR as plr_id, the district name and the geometrical shape of the area as a polygon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02642044",
   "metadata": {},
   "source": [
    "##### Polygon? What was that again?\n",
    "\n",
    "<img src=\"images/geometries.jpg\" alt=\"geometries\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c75e1",
   "metadata": {},
   "source": [
    "So, those polygons should give us areas of Berlin. Let's give it a try: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the geometries\n",
    "gdf.plot(color='grey', figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed600ba",
   "metadata": {},
   "source": [
    "That somehow looks like Berlin which makes us quite confident to proceed to try to merge the sets, since our bike theft data is not yet inside our geodataframe (or vice versa) - those are still two seperate data sets.  \n",
    "\n",
    "So - we need to have a look at the column that allow us to merge ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike thefts lor column\n",
    "thefts_df.lor.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ab8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geodataframe lor column\n",
    "gdf.plr_id.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cebe37",
   "metadata": {},
   "source": [
    "Not that easy, again.  \n",
    "- The column 'lor' in the bike theft data is an integer.  \n",
    "- Integers as numeric values can't have leading zeros.  \n",
    "- That's why it sometimes is 8 digits, sometimes is just 7 digits long - it then misses a leading 0 - we need to impute!  \n",
    "\n",
    "In the geodataframe, the lor column is an object, which means a string in this case.  \n",
    "Feel free to have a closer look ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the lor column datatype to string\n",
    "thefts_df['lor_str'] = thefts_df['lor'].astype('str')\n",
    "\n",
    "# fill leading gaps up to 8 characters with zeros and call the new column accordingly to the geodataframe\n",
    "thefts_df['plr_id'] = thefts_df['lor_str'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "# dropping no longer needed columns\n",
    "thefts_df.drop(columns=['lor', 'lor_str'], inplace=True)\n",
    "\n",
    "thefts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a49e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with the geodataframe\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef65cb",
   "metadata": {},
   "source": [
    "Now, we are able to merge our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78857af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataframes on the plr_id columns\n",
    "gdf_biketheft = gdf.merge(thefts_df, on='plr_id')\n",
    "gdf_biketheft.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db496c6",
   "metadata": {},
   "source": [
    "And so, we are finally able to infer infer the are with the most bikes stolen  \n",
    "by aggregating count of thefts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36d5ba",
   "metadata": {},
   "source": [
    "aggregating count of thefts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting thefts in areas\n",
    "df_plr_group_thefts = gdf_biketheft.groupby(by='plr_id').size().reset_index(name='thefts')\n",
    "\n",
    "# showing new dataframe with plr_id and aggregated count of thefts\n",
    "df_plr_group_thefts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276f0bc",
   "metadata": {},
   "source": [
    "aggregating average theft amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1738e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting thefts in areas\n",
    "df_plr_group_mean = gdf_biketheft.groupby(by='plr_id').mean().reset_index()\n",
    "df_plr_group_mean = df_plr_group_mean[['plr_id', 'schadenshoehe']]\n",
    "df_plr_group_mean = df_plr_group_mean.astype({'schadenshoehe': 'int64'})\n",
    "df_plr_group_mean.rename(columns={'schadenshoehe': 'avg_amount'}, inplace=True)\n",
    "\n",
    "# showing new dataframe with plr_id and aggregated mean of thefts\n",
    "df_plr_group_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the aggregates into the initial geodataframe\n",
    "gdf_biketheft = gdf.merge(df_plr_group_thefts, on='plr_id')\n",
    "gdf_biketheft = gdf_biketheft.merge(df_plr_group_mean, on='plr_id')\n",
    "gdf_biketheft.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, which area is the winner?\n",
    "gdf_biketheft[gdf_biketheft.thefts == gdf_biketheft.thefts.max()][['plr_name', 'thefts', 'avg_amount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a029d7e",
   "metadata": {},
   "source": [
    "And here we have our winner - it is __Alt-Treptow with 501 thefts__ in the observed timeframe with an average theft amount of 791 Euro!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef773c",
   "metadata": {},
   "source": [
    "Congratulations!  \n",
    "You made it through another intense notebook - but we hope the little excursions brought some fun ..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4771abb73651cc71498e03f3559c7e0f15f38d5124065b3832974a7bbffea7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nf_base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
